# 👋👋👋 hi. this is blog from zac skalko.

https://twitter.com/zapplebee

https://github.com/zapplebee/droplet

I have started way too many blogs and got caught up on the making it and not the writing.

this is just plain text for that reason.

---

i started setting up my droplet today.

here's a log.

i created a droplet on digital ocean today.

1 GB Memory / 25 GB Disk + 10 GB / SFO3 - Docker 23.0.6 on Ubuntu 22.04

I mounted the storage separately because i can easily move to another droplet if i choose.

with docker on it, i can just try a bunch of languages and images.

after getting it set up with ssh by public key from my mac mini at home, i logged in.

the lockfile for apt-get was broken on the first install.

i tried to remove the lock file and retry. no dice. i restarted the machine and it was fine.

the first real tool i installed was vs code server. it made it super easy to just drive this machine like it's my local machine.

after that, opened up http and https with ufw

```
ufw open http
ufw open https

```

i wanted a vert simple way to serve something and make sure i could reach the device from the internet, not just over ssh

i installed bun and made a very simple http handler.

```
curl -fsSL https://bun.sh/install | bash

```

dang zip was missing. i installed with

```
apt-get install zip
```

i figured i should install build essential too

```
apt-get install build-essential
```

now i could install bun.

and run

```
Bun.serve({
    port: 80, // defaults to $BUN_PORT, $PORT, $NODE_PORT otherwise 3000
    hostname: "0.0.0.0", // defaults to "0.0.0.0"
    fetch(req) {
      return new Response("h e l l o   w o r l d");
    },
  });

```

started it. i couldn't hit it from the internet.

digital ocean required that i also create a firewall rule

so i did. 22, 80, 443

hit it from the internet and got a response 💪

great. now i needed to set up dns.

i went to my trusty https://freedns.afraid.org/

set the old one that was pointing to my home server to the droplet's address

it took some time to propagate.

from chrome though, all i was getting was ERR_CONNECTION_REFUSED

i tried to curl it from my local machine

```
➜  ~ curl zapplebee.prettybirdserver.com
h e l l o   w o r l d%
```

i got back the expected response.

so what is going on with chrome.

i went to the network tab and copied the request as curl.

i decided that it might be something a browser call is enforcing, so i should try all the same everything (cookies headers etc) from another spot.

```
➜  ~ curl 'https://zapplebee.prettybirdserver.com/' \
  -H 'Upgrade-Insecure-Requests: 1' \
  -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36' \
  -H 'sec-ch-ua: "Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"' \
  -H 'sec-ch-ua-mobile: ?0' \
  -H 'sec-ch-ua-platform: "macOS"' \
  --compressed
curl: (7) Failed to connect to zapplebee.prettybirdserver.com port 443: Connection refused

```

i realized that chrome was trying to reach the https port 443. on which i had nothing running yet

but it was hitting the machine

just not in a way that i could see.

i could now set up certbot.

```
snap install --classic certbot
```

i dont really have a proxy or load balancer decided on yet, so i was going to just use certbot without auto configuration.

https://www.digitalocean.com/community/tutorials/how-to-use-certbot-standalone-mode-to-retrieve-let-s-encrypt-ssl-certificates-on-ubuntu-20-04

i created the certs manually

```
certbot certonly --standalone -d zapplebee.prettybirdserver.com

...
Successfully received certificate.
Certificate is saved at: /etc/letsencrypt/live/zapplebee.prettybirdserver.com/fullchain.pem
Key is saved at:         /etc/letsencrypt/live/zapplebee.prettybirdserver.com/privkey.pem
This certificate expires on 2024-04-12.

```

once i set up a service that needs to restart on renewal i can update this line in the conf file, `/etc/letsencrypt/renewal/zapplebee.prettybirdserver.com.conf`

```
renew_hook = systemctl reload your_service
```

now i have to look up how to serve with the certs from bun.

the simplest thing in the world.

```
Bun.serve({
  port: 443,
  hostname: "0.0.0.0",
  fetch(req) {
    return new Response("h e l l o   w o r l d");
  },
  tls: {
    cert: Bun.file(
      "/etc/letsencrypt/live/zapplebee.prettybirdserver.com/cert.pem",
    ),
    key: Bun.file(
      "/etc/letsencrypt/live/zapplebee.prettybirdserver.com/privkey.pem",
    ),
  },
});
```

now i just need to handle directing traffic to https

```

Bun.serve({
  port: 443,
  hostname: "0.0.0.0",
  fetch(req) {
    return new Response("h e l l o   w o r l d");
  },
  tls: {
    cert: Bun.file(
      "/etc/letsencrypt/live/zapplebee.prettybirdserver.com/cert.pem",
    ),
    key: Bun.file(
      "/etc/letsencrypt/live/zapplebee.prettybirdserver.com/privkey.pem",
    ),
  },
});

Bun.serve({
  port: 80,
  hostname: "0.0.0.0",
  fetch(req) {
    return Response.redirect(`${req.url.replace(/^http:/gi, "https:")}`, 302);
  },
});

```

https://regexr.com/ is my trusty regex helper site.

this was substantially easier than any proxy config i have ever used.

it's alive.

```

➜  ~ curl zapplebee.prettybirdserver.com -L --verbose
*   Trying 159.223.202.91:80...
* Connected to zapplebee.prettybirdserver.com (159.223.202.91) port 80 (#0)
> GET / HTTP/1.1
> Host: zapplebee.prettybirdserver.com
> User-Agent: curl/7.77.0
> Accept: */*
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 302 Found
< Location: https://zapplebee.prettybirdserver.com/
< Date: Sat, 13 Jan 2024 03:08:35 GMT
< Content-Length: 0
<
* Connection #0 to host zapplebee.prettybirdserver.com left intact
* Issue another request to this URL: 'https://zapplebee.prettybirdserver.com/'
*   Trying 159.223.202.91:443...
* Connected to zapplebee.prettybirdserver.com (159.223.202.91) port 443 (#1)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*  CAfile: /etc/ssl/cert.pem
*  CApath: none
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-ECDSA-CHACHA20-POLY1305
* ALPN, server did not agree to a protocol
* Server certificate:
*  subject: CN=zapplebee.prettybirdserver.com
*  start date: Jan 13 01:40:23 2024 GMT
*  expire date: Apr 12 01:40:22 2024 GMT
*  subjectAltName: host "zapplebee.prettybirdserver.com" matched cert's "zapplebee.prettybirdserver.com"
*  issuer: C=US; O=Let's Encrypt; CN=R3
*  SSL certificate verify ok.
> GET / HTTP/1.1
> Host: zapplebee.prettybirdserver.com
> User-Agent: curl/7.77.0
> Accept: */*
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< content-type: text/plain;charset=utf-8
< Date: Sat, 13 Jan 2024 03:08:35 GMT
< Content-Length: 21
<
* Connection #1 to host zapplebee.prettybirdserver.com left intact
h e l l o   w o r l d%

```

---

the first bug was discovered.

by default, bun did not read the encoding of the file that was read from disk.

i guess i assumed it would.

my good friend Ryan Rampersad https://twitter.com/ryanmr pointed out that the unicode characters where buggy.

i looked and saw that the server was responding with a `Content-type` header of `text/markdown`.

Bun had identified this but did not automatically set the additional encoding information.

it was changes to `text/markdown; charset=utf-8`

https://github.com/zapplebee/droplet/commit/bb3e0c4b7f4860ed59d7c6789d2ba8752b24614a

now it works as expected.

---

i need a way to run this locally that ignores the certs

the way it will work is based on the NODE_ENV, i'll create the configuration differently.

though i don't love that i am going to bake this directly into the entrypoint file, the app is still very simple and it's okay. the goal is to make some thing that can be changed.

in the .bashrc file i'll export the NODE_ENV.

```
export NODE_ENV=production
```

that way, anytime i am running something on this machine it will automatically be set.

right now the service is just being run in the background.

```
bun index.ts &
```

While bun does support a watch mode, i dont really want to think about memory leaks yet so i am just going to make a handy alias to stop and restart it.

to find the running process in the background, I'm running

```
ss -lptn 'sport = :80'
State                 Recv-Q                Send-Q                               Local Address:Port                               Peer Address:Port               Process
LISTEN                0                     512                                        0.0.0.0:80                                      0.0.0.0:*                   users:(("bun",pid=1562,fd=14))
```

while this is good, i'm going to need to trim that output so i can get just the process id and kill it

i can use `awk` for this.

```
ss -lptn 'sport = :80' | awk 'NR > 1 {print $6}'
users:(("bun",pid=1562,fd=14))

```

this skips the first line, the table headers, and then takes the sixth column value.

still not quite just the process id.

a couple cuts will do the trick

```
ss -lptn 'sport = :80' | awk 'NR > 1 {print $6}'  | cut -d= -f2 | cut -d, -f1
1562
```

this gives me just the process id of the server that is listening on port 80. i could have used the https port 443 instead but this is fine since it has to serve both for the redirects.

next i have to pass that var into a `kill` command.

I can do that with a command substitution.

```
kill $(ss -lptn 'sport = :80' | awk 'NR > 1 {print $6}'  | cut -d= -f2 | cut -d, -f1)
```

after that i just need to start the service again.

```

kill $(ss -lptn 'sport = :80' | awk 'NR > 1 {print $6}'  | cut -d= -f2 | cut -d, -f1) & bun /mnt/volume_sfo3_01/apps/helloworld/index.ts &

```

i'll add that as an alias in the `.bashrc` file

```
alias restartbun="kill $(ss -lptn 'sport = :80' | awk 'NR > 1 {print $6}'  | cut -d= -f2 | cut -d, -f1) & bun /mnt/volume_sfo3_01/apps/helloworld/index.ts &"
```

this gives me a couple advantages right now.

1. there's no auto deploy. i have to pull from git or change the files on the server and restart. this will let me make a mess of things and not have an auto watcher restarting the server if it's not in a good state.
2. it's based on what port is exposed, so if i change the actual edge listener to a proxy or something, i can restart it with pretty much the same command.

---

okay, now i can finally set up the application code to serve certs if we're in production mode.

```

const PRODUCTION_CONFIG = {
  port: 443,
  tls: {
    cert: Bun.file(
      "/etc/letsencrypt/live/zapplebee.prettybirdserver.com/cert.pem",
    ),
    key: Bun.file(
      "/etc/letsencrypt/live/zapplebee.prettybirdserver.com/privkey.pem",
    ),
  },
} as const;

const DEV_CONFIG = {
  port: 3100
} as const;

const IS_PRODUCTON = process.env.NODE_ENV === 'production';

const LIVE_CONFIG = IS_PRODUCTON ? PRODUCTION_CONFIG : DEV_CONFIG;

Bun.serve({

  hostname: "0.0.0.0",
  fetch(req) {
    return new Response(Bun.file("/mnt/volume_sfo3_01/apps/notes/setting-up-the-droplet.md"), {
      headers: {
        'Content-type': 'text/markdown; charset=utf-8'
      }
    });
  },
  ...LIVE_CONFIG
});


if(IS_PRODUCTON) {
  // no need to serve the redirects if we're not in prod
  Bun.serve({
    port: 80,
    hostname: "0.0.0.0",
    fetch(req) {
      return Response.redirect(`${req.url.replace(/^http:/gi, "https:")}`, 302);
    },
  });

}


```

here's the changes

https://github.com/zapplebee/droplet/commit/73933eee9417f10c9139d4875cc6f292696ff9ab

---

after setting this up i realized i dont have prettier installed in the droplet, and since i am doing a lot of authoring directly on it via ssh, i might want to do that in the future.

// TODO set up prettier and git hooks i guess

---

okay now that i have a dev mode i can start to actually add some features to this thing.

first of all, i would prefer some minimal styling...

and i do mean minimal.

and to actually serve this as html

so far i don't have any dependencies and it might be good to keep it that way for a while.

bun gives me a lot out of the box and would like to keep dependencies to a minimum

first, i'll read up all the markdown paths.

```
export async function getFilePaths(): Promise<Array<string>> {
  const glob = new Glob("*.md");

  const filepaths: Array<string> = [];

  for await (const file of glob.scan(NOTES_DIRECTORY)) {
    filepaths.push(`${NOTES_DIRECTORY}${file}`);
  }

  return filepaths;
}

```

then i'll mash em together for now

```
export async function getAsHtml(): Promise<string> {
  const filepaths = await getFilePaths();
  const files = filepaths.map((e) => Bun.file(e));

  const fileContents = await Promise.all(files.map((e) => e.text()));

  const rawBody = fileContents.join("\n---\n");

  return `<!DOCTYPE html>
<html><body><style>* {background-color: black; color: green;}</style><pre>${Bun.escapeHTML(rawBody)}</pre></body></html>`;
}

```

at last i'll import it in the index.

since i want this to fail early, ie at start up, i'll use bun's macro capability to do it.

```
import { getAsHtml } from "./files" with { type: "macro" };
```

this runs the getting and escaping of the files at start up and in-lines the result into the AST.

https://bun.sh/docs/bundler/macros

not super useful now since i am not bundling, but an appropriate use of a macro

i also added gzip it was very simple.

```
const HTML_CONTENT = await getAsHtml();

const data = Buffer.from(HTML_CONTENT);
const compressed = Bun.gzipSync(data);

Bun.serve({
  hostname: "0.0.0.0",
  fetch(req) {
    return new Response(compressed, {
      headers: {
        "Content-Encoding": "gzip",
        "Content-type": "text/html; charset=utf-8",
      },
    });
  },
  ...LIVE_CONFIG,
});

```

---

although I do really like this just one long file method,
there's something not quite right about it.

and that's the ability to link to a specific piece of content.

I think that the best course of action is just to create id anchors for every line.

then one can easily deep link directly to a place on the page.

there's a couple things that need to happen in order for that to work as expected.

1. i need to split the file line by line.
2. map those back together with a link at the start of the line.
3. create a line count column on the left site that has a link to the line itself.

as i was doing this i thought i might need to start to tag where i am in the git repo so that if someone (me) wanted to follow along in the future, it would be easy

```
git show-ref
3590fff8df7ecfcebd7f9379637977403549f62e refs/heads/main
3590fff8df7ecfcebd7f9379637977403549f62e refs/remotes/origin/HEAD
3590fff8df7ecfcebd7f9379637977403549f62e refs/remotes/origin/main
```

i sure could just let people do a git blame. but they might just be reading this as plain text.

a few edits to the html file and bang we we have direct links.

i am fighting my a lot of my instincts to not bring in react at this stage as the markup is getting just a bit complex.

as my friend ardeshir (https://hachyderm.io/@sepahsalar) said:

"Yep, you start adding A 🏷️ s and next thing you know, you’ve written a new RSC framework"

```

export async function getAsHtml(): Promise<string> {
  const filepaths = await getFilePaths();
  const files = filepaths.map((e) => Bun.file(e));
  const fileContents = await Promise.all(files.map((e) => e.text()));
  const rawBody = fileContents.join("\n---\n").replaceAll("\r", "");
  const escapedBody = Bun.escapeHTML(rawBody);
  const bodyLines = escapedBody.split("\n");
  const maxCharactersInLineNumber = String(bodyLines.length).length;

  return `<!DOCTYPE html>
<html><body><style>* {background-color: black; color: #4d9c25;} .line-link {color: #2f5c19;} .space, .line-link {  -webkit-user-select: none; -ms-user-select: none; user-select: none;}</style><pre>${bodyLines
    .map((line, index) => {
      const lineNumber = String(index).padStart(maxCharactersInLineNumber, "0");
      const lineId = `line-${lineNumber}`;

      return `<span class="line" id="${lineId}"><a class="line-link" href="#${lineId}">${lineNumber}</a><span class="space">&nbsp;&nbsp;&nbsp;&nbsp;</span><span>${line}</span></span>`;
    })
    .join("\n")}</pre></body></html>`;
}


```

the next thing i want to do is enable a line wrap mode for mobile.

but i dont want the markdown style codeblocks wrap.

so i first need to make the codeblocks wrapped in a new node.

i'll have to do some refactoring of the page builder.

as soon as I change styles in there to a static CSS file, i was already starting to get irritated by the lack of structure in code.

there's literally one handler. it should be easy to change, and it is, the hard part is thinking about what I want to change it to.

i was really trying defer design decisions because, well, i dont want to think about it too hard.

this is supposed to be a fun little project.

i guess to keep this as simple as possible, i should just write vanilla css and i really need to vanilla js and just author and host them from a public folder

It's time that I actually messed with these Link headers.

This should allow the network requests to start as soon as the document loads.

https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Link

That will keep the doc in cache until it used by the page.

I need to modify the response that the html request returns

```
return new Response(HTML_RESPONSE_BODY, {
  headers: {
    "Content-Encoding": "gzip",
    "Content-type": "text/html; charset=utf-8",
    Link: `</public/main.css>; rel="prefetch"; as="style";`,
  },
});
```

Then, in the actual CSS in the HTML, I can import it for free.

and it's already been loaded

---

added a little magic to the html renderer. not my best code ever. but it does the job until i find a real framework i want to use for this.

````

export async function getAsHtml(): Promise<string> {
  const filepaths = await getFilePaths();
  const files = filepaths.map((e) => Bun.file(e));
  const fileContents = await Promise.all(files.map((e) => e.text()));
  const rawBody = fileContents.join("\n---\n").replaceAll("\r", "");
  const escapedBody = Bun.escapeHTML(rawBody);
  const bodyLines = escapedBody.split("\n");
  const maxCharactersInLineNumber = String(bodyLines.length).length;

  let inCodeBlock = false;

  return `<!DOCTYPE html>
<html><head></head><body><style>@import "/public/main.css";</style><pre>${bodyLines
    .map((line, index) => {
      const linkedLine = line.replaceAll(
        /(https:\/\/[^\s\)]+)/gi,
        '<a href="$&">$&</a>'
      );
      const lineNumber = String(index).padStart(maxCharactersInLineNumber, "0");
      const lineId = `line-${lineNumber}`;

      const isBackticks = line.trim() === "```";

      if (isBackticks) {
        inCodeBlock = !inCodeBlock;
      }

      const inCode = Boolean(inCodeBlock || isBackticks);

      return `<div class="line" id="${lineId}"><a class="line-link" href="#${lineId}">${lineNumber}</a><span class="space">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="${inCode ? "codeblock" : ""}">${inCode ? line : linkedLine}</span></div>`;
    })
    .join("\n")}</pre></body></html>`;
}

````

just a little magic

---

whoops i wrote a bunch of code without documenting it.

i just needed to experiment with the styling.

I actually should review and refactor at some point because there is too many
dom nodes for what i am trying to do here.

---

I refactored the render function a little to make it more readable.

Broke the massive template string into a few variables and mashed them together.

Variable names are free documentation.

````
export async function getAsHtml(): Promise<string> {
  const filepaths = await getFilePaths();
  const files = filepaths.map((e) => Bun.file(e));
  const fileContents = await Promise.all(files.map((e) => e.text()));
  const rawBody = fileContents.join("\n---\n").replaceAll("\r", "");
  const escapedBody = Bun.escapeHTML(rawBody);
  const bodyLines = escapedBody.split("\n");
  const maxCharactersInLineNumber = String(bodyLines.length).length;

  let inCodeBlock = false;

  const headTags = `
<title>zapplebee.prettybirdserver.com</title>
<style>@import "/public/main.css";</style>
`;

  const head = `<!DOCTYPE html>
<html><head>${headTags}</head><body><main>`;

  const tail = `</main></body></html>`;

  const main = bodyLines.map((line, index) => {
    const lineNumber = String(index).padStart(maxCharactersInLineNumber, "0");
    const lineId = `line-${lineNumber}`;

    const isBackticks = line.startsWith("```");

    let addCodeBlockOpenTag = false;
    let addCodeBlockCloseTag = false;

    if (isBackticks && !inCodeBlock) {
      addCodeBlockOpenTag = true;
    }

    if (isBackticks) {
      inCodeBlock = !inCodeBlock;
    }

    if (!inCodeBlock && isBackticks) {
      addCodeBlockCloseTag = true;
    }

    const inCode = Boolean(inCodeBlock || isBackticks);

    const lineText = inCode
      ? line
      : line.replaceAll(/(https:\/\/[^\s\)]+)/gi, '<a href="$&">$&</a>');

    const containerOpenTag = addCodeBlockOpenTag
      ? `<div class="codeblock-container"><div class="codeblock-wrapper">`
      : "";

    const openingLineTag = `<div class="line" id="${lineId}">`;
    const lineIdxElement = `<a class="line-link" href="#${lineId}">${lineNumber}</a>`;
    const lineStrElement = `<span class="${inCode ? "codeblock" : "prose"}">${lineText}</span>`;
    const closingLineTag = `</div>`;
    const containerCloseTag = addCodeBlockCloseTag ? `</div></div>` : "";

    return [
      containerOpenTag,
      openingLineTag,
      lineIdxElement,
      lineStrElement,
      closingLineTag,
      containerCloseTag,
    ].join("");
  }).join("");

  return [head, main, tail].join("");
}

````

as for the styles. i found a new property that i have never run into before

```
  -moz-text-size-adjust: none;
  -webkit-text-size-adjust: none;
  text-size-adjust: none;
```

I could not figure out what was making my fonts all messed up
despite the `!important` tag.

This is what I get for using somebody else's CSS reset all the time.

Finally made the CSS into something i could tolerate.
It's interesting how dependant I have become on CSS-in-JS or tailwind

This was harder than I remember it being.

```

:root {
  --textwidth: 40ch;
  --linenumberwidth: 3ch;
  --gapwidth: 2ch;
}

@media (min-width: 500px) {
  :root {
    --textwidth: 80ch;
  }
}

* {
  padding: 0;
  margin: 0;
  box-sizing: border-box;
  line-height: 1.2rem;
  font-size: 16px;
  font-weight: 400;
  color: #4d9c25;
  -moz-text-size-adjust: none;
  -webkit-text-size-adjust: none;
  text-size-adjust: none;
}

body {
  background-color: black;
  display: block;
}

main {
  font-family: monospace;
  width: calc(var(--textwidth) + var(--linenumberwidth) + var(--gapwidth));
  margin: auto;
  display: block;
}

.line {
  display: flex;
  flex-direction: row;
  gap: var(--gapwidth);
}
.line-link {
  color: #2f5c19;
  -webkit-user-select: none;
  -ms-user-select: none;
  user-select: none;
  white-space: pre;
  width: var(--linenumberwidth);
}

.codeblock-container {
  width: calc(var(--textwidth) + var(--linenumberwidth) + var(--gapwidth));
  overflow-x: auto;
  overflow-y: hidden;
  background-color: rgb(63, 63, 63);
}

.codeblock {
  color: rgb(215, 246, 152);
  white-space: pre;
}

.prose {
  width: var(--textwidth);
  white-space: pre-wrap;
  overflow-wrap: break-word;
}

::-webkit-scrollbar {
  height: 1rem;
  width: 1ch;
  background: rgb(63, 63, 63);
}

::-webkit-scrollbar-thumb {
  background: rgb(215, 246, 152);
}

::-webkit-scrollbar-corner {
  background: rgb(63, 63, 63);
}

```

---

the next thing i am going to have to figure out for this is images.

they're tricky for a couple reasons.

1. the style of this website doesn't really have anything that spans too
   much vertical space
2. they cost bandwidth in a way that the plain text just doesn't.
3. i am trying to not load and js in the client for performance reasons
   if i want to do any tricks to save bandwidth or just allow a peek at the image
   that's going to be tough without js

I have a bit of an idea

I could load like, two lines of the image, and add a button to expand.

like this

<!-- prettier-ignore-start -->

imageurl: an-image.jpg
███████████████████████
███████████████████████
███ click to expand ███


imageurl: an-image.jpg
██████████████████████
██████████████████████
██████████████████████
███   █████████  █████
██████████████████████
█  ██████████████  ███
███  ██████████   ████
████            ██████
██████████████████████
██████████████████████
███ click to close ███

<!-- prettier-ignore-end -->

that could be done with just a checkbox and some custom styles

but it wouldn't be very accessible.

so far, as strange as the set of this site is, it is rather accessible.

though i could bump up the contrast from the bg and the text,

there's not a busy menu to navigate through via keyboard.

there's no animations.

theres' no dynamic loading of content.

the other accessibility thing I should do is probably add image roles for the emoji

https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Roles/img_role

(exactly unlike above...)

i would also like a way to be able to add content from my phone.

this is meant, in part, as a replacement for _centralized microblogging platform formerly known as twitter_
in case you couldn't tell from my typing voice. haha.

i tried mastodon, but i just didn't get it or something.

so the ability to add content whenever, whereever would be nice.

stretch goal would be to add the ability to comment on lines.

one of the goals of this site is to practice progressive by incremental improvement of this feed.
backwards compatibility too.

since right now this is just a giant markdown file,
it could obviously be replatformed.

the other goal is that all the changes to the site get logged here.

think of it as a git log + architecture decision record + readme + bullet journal + social media

the simple form of it has let me write almost 1000 lines already!

in fact

TODO: fix the fact that right now the line number width is fixed to three characters

i'd also like some kind of convention for being able to write long form segments that get their own page

something like

<POST slug="first-actual-post" summary="This is the first real post other than the endless log">
# hello world.

this is the first actual blog post of the site. the log is there for everything but
sometimes i will want to be able to write specific content.
</POST>

honestly, i think that i'm happy enought with that as a pattern

the landing page of the site should show something like the last 10 lines of
the log with a link to the full log

and then posts as links.

---

i was trying to set some html meta tags.

as usual, i stole from ryan

https://github.com/ryanmr/blog/blob/main/src/components/BaseHead.astro

---

<POST slug="the-react-empire" description="Thoughts on the React ecosystem as it enters its naidir of simplicity">

I recently read this post from [Cassidy Williams](https://twitter.com/cassidoo) on [being a little disappointed with React](https://blog.cassidoo.co/post/annoyed-at-react/)

An awful lot of it resonated with me so I thought I would try to put my own thoughts into words on it.

When I was just a little computer boy, there was _no_ good way to write real application code that lived in the browser.

It was either a ton of boilerplate `createElement` or JQuery. And while both were really powerful, they required a lot of code to do some pretty plain things by today's standards.

And they were imperative, rather than declarative.

Meaning instead of indicating how you would want the DOM to look, you were giving instructions about how to mutate the DOM.

Eventually, I discovered AngularJS. At this time I was still in my adolescence with real software. So it was an amazing journey to be able to write JavaScript apps by just adding a new script tag from a CDN url.

I was still writing apps in Notepad++ (Or maybe it was already Sublime Text by then) and FTPing them to the server.

Templates for the way the DOM was described in AngularJS was [based on strings](https://docs.angularjs.org/guide/component#components-as-route-templates) or XHR requests to other HTML documents with custom directives specifically to attach into the AngularJS state and events.

This was a huge win for me. I was very tired of creating elements, or homebrewing my own abstractions for creating elements.

It did have one weakness though. There wasn't a lot of help to make sure what you were writing would work as expected since the templates were just strings. You kind of just had to run it and make sure that you connected your state and handlers correctly.

And I was still writing a lot of code to manage data going up and down the tree.

Ultimately, it did still make it much easier to work with. AND I STILL DIDN'T NEED ANYTHING BUT TO UPLOAD THE FILES.

I built a lot in AngularJS.

At the time I was running a couple PHP servers and with some local databases.

I had tried with Java and a few other languages, but as an independently taught dev, it was really hard to go from zero knowledge to a running server.

🫠 figuring out a build toolchain and a compiler when you are still working hard to remember that environmental variables are set per shell.
😎 file makes webpage.

It was simple. Simple was King (and still is).

Enter React.

By now, I was pretty comfortable in my space and had started to experiment with NodeJS.

Wouldn't it be cool if the whole stack was the same language? That would make skills I learned in one place make me even stronger. I was still leaning about the language after all (and still am -- it's constantly evolving).

I went to a local conference and everyone was talking about React.

It was mesmerizing what they could build with a little bit of code. But surely I could never get a foothold into this, I mean that wasn't JS they were writing, right?

Yes but no.

JSX was transformative. Despite it being just a syntax over a `createElement` function, it totally changes the way that one could reason about the DOM tree.

It was a good abstraction. It made the important information very visible and made the irrelevant (read as boilerplate) almost disappear.

The transformation that it enabled in reasoning with templates in JS also required another transformation. Source JSX -> JavaScript ready for the browser.

Today, there is a lot more in the ecosystem that could help you do this kind of transform. At the time there really wasn't, and even if there was I would don't think I could set that up from scratch at the time. Even by following step-by-step instructions.

The real magic, was that React also published `create-react-app`. An all-things-included bundler, application scaffold, example application that could easily start to grow from a single page to whatever you wanted it to be -- all while hiding the magic that transformed JSX to JS.

A person could get addicted to the ergonomics of the framework before ever having to think about the how.

Eventually, like all computer people, I became too curious for my own good and ejected a React App.

Ejecting a `create-react-app` would take all the behind-the-scenes magic that was happening and plop it into your work directory.

I couldn't get enough -- webpack, babel, the whole thing. I learned about ASTs, I learned about compilers. It was a magical time of growth for me. It didn't hurt that at the time I was working at a newish job where the team's ethic was "let the people focus".

Despite all this being in front of me, it would still take a ton to learn before I could really effect change with that configuration.

And while I was figuring that out, my app would still just build and run. **The app still felt simple**. I was never more productive in my life.

Then came hooks.

We no longer had to utilize so many language features to interact with the state and the DOM. In fact we barely had to learn about the framework and its lifecycle to be able to write code for it.

An even better refinement on the abstraction. A truly good abstraction should enable you to look at a piece of code and see what its intent is.

```js
const [isOn, setIsOn] = useState(false);
```

Even if you've never seen JavaScript, you may be able to reason about that.

The patterns let you think about what you're doing. Not what the framework is doing.

---

There is unquestionably a new age for React. It's not just React Server Components. It's a new world that it lives in.

The primary, suggested. ways to start a React project is a tool is not maintained as side-by-side with React itself.

https://react.dev/learn/start-a-new-react-project#production-grade-react-frameworks

React is no longer suggesting that you start with the simplest way to build and deploy and gradually add more complexity. Instead, you get all that complexity on day one.

---

Let's take Next as the focal point of the rest of this discussion. Because, like it or not, it is become a defacto _next_ step.

We now have must specify a boundry about where our components can run.

```js
"use client";
```

Meaning that we have to constantly think about if a piece of code has access to the DOM in a browser or not.

This is all at the file level and not the component level meaning that the handy co-location of mini components gets broken.

Routing is based on a filesystem convention, or is it? Next already has two divergent page path conventions.

And while it purports to be a full-stack framework, God help you if you want to use something you learned in Node, because sometimes you have access to that API and sometimes you don't.

Next, and the other full-stack React frameworks have taken us back to having to think about what the framework is doing. It's a step backwards for development experience in a big way.

---

Complexity should only ever be added to an application as a decision of the product that is building that application. React has abandoned this mentality.

It's still totally possible to build or use React for little pieces of your website, but it's not even approached as a concept in the docs.

I still reach for a React app (not `create-react-app`, usually with Vite now) when I want to build something with any amount of rapidity. But less so every day -- and certainly not in the way that it is described in it's documentation.

JSX will survive this curve away from the simplicty of a client-side React application. Whether or not React remains king is uncertain. Remember, simplicity is king.

</POST>

as i was writing this post i got a little distracted by making it fancy.

but i needed someway to deep link it.

i decided that while the hashes did work, it wasn't great for the future if i ever want to capture where traffic is going

so instead now i one single line of javascript on the client.

```
  let scrollToScript = "";
  if (focusId && ids.has(focusId)) {
    scrollToScript = `<script>window['${focusId}'].scrollIntoView(true);</script>`;
  }
```

---

while trying to get meta tags working i discovered that there was a certificate problem that was preventing crawlers from getting to the page.

so i decided it was time to move from the https://freedns.afraid.org/ subdomain i was using and buy my own

https://zapplebee.online.com

i didnt wait long enough for DNS to propagate before trying to set up certs.

so now i have to wait an hour

```

certbot certonly --standalone -d zapplebee.online
Saving debug log to /var/log/letsencrypt/letsencrypt.log
Requesting a certificate for zapplebee.online
An unexpected error occurred:
Error creating new order :: too many failed authorizations recently: see https://letsencrypt.org/docs/failed-validation-limit/

```

once that's passed, i should be able to reinstall new certs.

_AND_ then i'll have to figure out all the places i hard coded the domain.

it was a good exercise to change it this early for that reason.

i bought the domain from https://www.namecheap.com/

"bought". I lease the domain from namecheap.

that's how a registrar works.

I set an A record to point to my droplet

A @ 159.223.202.91

this points all root level traffic to the droplet.

a little sad to leave behind the old prettybird url, but i didn't have really to many rights over it with freedns.

I am interested in Bun will allow me to serve multiple domains from a single process.

It does include a `hostname` param in the set up for the listener, so i dont see why not.

it would just have to be in the same process because they'll both be using ports 80 and 443

---

okay i finally got through some real mess on getting this started in docker.
i was mounting the `/etc/letsencrypt/live` into the container so that i could serve certs.
but it turns out, that those live certs are symlinked from `/etc/letsencrypt/archive`
(with an additional change of name fullchain.pem => fullchain1.pem).
after mounted the archive, docker operate on the certs just fine.
https://github.com/zapplebee/droplet/commit/6aca9c3291bf0727227c81ebb552c55cfae20dd6

---

after making the React post, i really wanted to enable comments.

considering just asking people to make a PR.

thinking about adding dates to the lines.

```
git blame -L 1,1000 -- notes/2024-01.md | awk '{print $5}'

```

that would give me the date modified of all the lines on the file.

---

i shared one of my internal-at-work blog posts with someone today.

it was exactly what they needed for the problem they were trying to figure out.

i need to write in there more.

<📚 title="Ada's Algorithm: How Lord Byron's Daughter Ada Lovelace Launched the Digital Age" >

I am liking this book a lot.

I'm about 2/3rd through it.

It's a semi epistolary biography of Ada Lovelace.

It focuses on her work with Charles Babbage on his Differential and Analytical Engine.

It did spend a little too much time on her family before she really became the focus of the story. Even then it is spending a lot of time with Babbage.

Still, I am enjoying it.

Reading the letters between the people in the book was a bit of an inspiration for me to start writing this log.

</📚>

NEW KEYBOARD DAY!

It's a new Keychron.

This one is a C2 Full Size Mechanical Keyboard.

Totally classic color scheme. All white/gray with no lights. And nice feeling brown switches.

I really have to figure out how i want to upload and render images on this log.

---

I added structured logs to the webserver.

The docker agent on the machine already records them to a file

this will show them, since i am just running one container

```
docker inspect --format='{{.LogPath}}' $(docker ps -q)
```

here's a sample output

```
{"log":"{\"hostname\":\"zapplebee.online\",\"ipAddress\":{\"address\":\"███REDACTED███\",\"family\":\"IPv4\",\"port\":███REDACTED███},\"level\":\"http\",\"message\":\"GET: /wp-login.php\",\"method\":\"GET\",\"pathname\":\"/wp-login.php\",\"search\":\"\",\"service\":\"helloworld\",\"timestamp\":1705453727099,\"userAgent\":\"Mozilla/5.0\"}\n","stream":"stdout","time":"2024-01-17T01:08:47.100782281Z"}
```

Someone trying to read my `/wp-login.php` 😂😂😂

I tried to curl them back but no dice.

---

Interesting.

I am seeing that that Bun is having trouble parsing a URL that got passet the request.

```
"10 | export async function mainFetchHandler(req: Request, server: Server) {\n"
"11 |   const requestUrl = new URL(req.url);\n"
"                          ^\n"
"TypeError: \"stager64\" cannot be parsed as a URL.\n"
"      at /apps/helloworld/main.ts:11:22\n"
"      at mainFetchHandler (/apps/helloworld/main.ts:10:40)\n"

```

I don't know how it was able to receive this.

It was easy to see logs from the container that were not published by the formatted logger.

```
cat $(docker inspect --format='{{.LogPath}}' $(docker ps -q)) | grep "helloworld" --invert-match -B 3 -A 8 | jq ".log"

```

This specifically inverts the match of "helloworld" that is a piece of default metadata that should always get posted in the log.

Then uses `jq` to get the log value.

